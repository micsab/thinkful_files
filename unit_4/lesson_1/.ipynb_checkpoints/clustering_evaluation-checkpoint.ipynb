{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "data_path = (\"http://archive.ics.uci.edu/ml/machine-learning-databases/\"\n",
    "             \"heart-disease/processed.cleveland.data\")\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Make sure the number of rows divides evenly into four samples\n",
    "rows = df.shape[0] - df.shape[0] % 4\n",
    "df = df.iloc[:rows, :]\n",
    "\n",
    "# Break into a set of features and a variable for the known outcome\n",
    "X = df.iloc[:, :13]\n",
    "y = df.iloc[:, 13]\n",
    "\n",
    "# Replace some random string values\n",
    "X = X.replace(to_replace='?', value=0)\n",
    "\n",
    "# Binarize y so that 1 means heart disease diagnosis and mean no diagnosis\n",
    "y = np.where(y > 0, 0, 1)\n",
    "\n",
    "# Normalize\n",
    "X_norm = normalize(X)\n",
    "\n",
    "# Data frame to store features and predicted cluster memberships\n",
    "ypred = pd.DataFrame()\n",
    "\n",
    "# Create the two-feature PCA for graphing purposes\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_norm)\n",
    "\n",
    "# Split the data into four equally-sized samples. First we break it in 1/2\n",
    "X_half1, X_half2, X_pcahalf1, X_pcahalf2 = train_test_split(\n",
    "    X_norm,\n",
    "    X_pca,\n",
    "    test_size=0.5,\n",
    "    random_state=42)\n",
    "\n",
    "# Then we halve the halves\n",
    "X1, X2, X_pca1, X_pca2 = train_test_split(\n",
    "    X_half1,\n",
    "    X_pcahalf1,\n",
    "    test_size=0.5,\n",
    "    random_state=42)\n",
    "X3, X4, X_pca3, X_pca4 = train_test_split(\n",
    "    X_half2,\n",
    "    X_pcahalf2,\n",
    "    test_size=0.5,\n",
    "    random_state=42)\n",
    "\n",
    "# Pass a list of tuples and a counter that increments each time we go\n",
    "# through the loop. The tuples are the data to be used by k-means,\n",
    "# and the PCA-derived features for graphing. We use k-means to fit a\n",
    "# model to the data, then store the predicted values and the two-feature\n",
    "# PCA solution in the data frame.\n",
    "for counter, data in enumerate([\n",
    "    (X1, X_pca1),\n",
    "    (X2, X_pca2),\n",
    "    (X3, X_pca3),\n",
    "    (X4, X_pca4)]):\n",
    "    \n",
    "    # Put the features into ypred\n",
    "    ypred['pca_f1' + '_sample' + str(counter)] = data[1][:, 0]\n",
    "    ypred['pca_f2' + '_sample' + str(counter)] = data[1][:, 1]\n",
    "    \n",
    "    # Generate cluster predictions and store them for clusters 2 to 4.\n",
    "    for nclust in range(2, 5):\n",
    "        pred = KMeans(n_clusters=nclust, random_state=42).fit_predict(data[0])\n",
    "        ypred['clust' + str(nclust) + '_sample' + str(counter)] = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
